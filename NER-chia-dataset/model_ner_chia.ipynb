{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jlopetegui98/Creation-of-a-synthetic-dataset-for-French-NER-in-clinical-trial-texts/blob/main/NER-chia-dataset/model_ner_chia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY3TjPTXUlAE"
      },
      "source": [
        "**Multilingual NER model trained over [Chia dataset](https://figshare.com/articles/dataset/Chia_Annotated_Datasets/11855817)**\n",
        "\n",
        "We are going to train a BERT based multilingual language model over the Chia dataset in english and then we will use this model to create the synthetic version of the dataset in French. Our idea is supported by the experiments already done with [multiNERD](https://huggingface.co/datasets/Babelscape/multinerd) dataset for multilingual NER in English and French."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxtbZfscUlAG"
      },
      "source": [
        "**Entities selection**\n",
        "\n",
        "Among all the entities in the dataset, we are going to focus for this project on the most represented ones. Then, we are just going to consider those entities with more than 1000 samples in total."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment if working in colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CvM3VFrnU8n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U datasets"
      ],
      "metadata": {
        "id": "S0_qbci4WXFV",
        "outputId": "541f5bb6-9b90-4147-f3a6-bce38de45cba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVTVWDvoUlAH",
        "outputId": "f74e5a14-17e7-49d1-d117-167d6b52d023"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/javierlopetegui/miniforge3/envs/honlp/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/Users/javierlopetegui/miniforge3/envs/honlp/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
        "import os\n",
        "from preprocessing_dataset import *\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "import json\n",
        "from datasets.features import ClassLabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHlUbwWhUlAI"
      },
      "outputs": [],
      "source": [
        "# dict for the entities (entity to int value)\n",
        "sel_ent = {\n",
        "    \"O\": 0,\n",
        "    \"B-Condition\": 1,\n",
        "    \"I-Condition\": 2,\n",
        "    \"B-Value\": 3,\n",
        "    \"I-Value\": 4,\n",
        "    \"B-Drug\": 5,\n",
        "    \"I-Grug\": 6,\n",
        "    \"B-Procedure\": 7,\n",
        "    \"I-Procedure\": 8,\n",
        "    \"B-Measurement\": 9,\n",
        "    \"I-Measurement\": 10,\n",
        "    \"B-Temporal\": 11,\n",
        "    \"I-Temporal\": 12,\n",
        "    \"B-Observation\": 13,\n",
        "    \"I-Observation\": 14,\n",
        "    \"B-Person\": 15,\n",
        "    \"I-Person\": 16\n",
        "}\n",
        "entities_list = list(sel_ent.keys())\n",
        "sel_ent_inv = {v: k for k, v in sel_ent.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh4vLMb8UlAI"
      },
      "outputs": [],
      "source": [
        "# data paths\n",
        "root_path = './' # comment if working on colab\n",
        "root_path = './drive/MyDrive/HandsOn-NLP'\n",
        "data_path = f'{root_path}/data'\n",
        "chia_bio_path = f\"{data_path}/chia_bio\"\n",
        "chia_prep_path = f\"{data_path}/chia_prep\"\n",
        "models_path = f\"{root_path}/models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I82X0otlUlAJ"
      },
      "outputs": [],
      "source": [
        "# preprocessing dataset to get the data in the right format for dataset entity creation\n",
        "preprocessing_dataset(chia_bio_path, output_path=chia_prep_path, labels2int = sel_ent, int2labels = sel_ent_inv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwzgyhJ5UlAJ",
        "outputId": "f88b138f-271a-4fef-b09b-e818aa591ccd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['NCT03363295_inc.bio.json',\n",
              " 'NCT02607319_inc.bio.json',\n",
              " 'NCT02102243_inc.bio.json',\n",
              " 'NCT03015818_exc.bio.json',\n",
              " 'NCT02226887_exc.bio.json']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# read the data after preprocessing\n",
        "files = os.listdir(chia_prep_path)\n",
        "files[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCO9RxNAUlAJ"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "\n",
        "for file in files:\n",
        "    with open(f\"{chia_prep_path}/{file}\", \"r\") as f:\n",
        "        stc = json.load(f)\n",
        "        sentences.extend(stc[\"sentences\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7W-lnFa-UlAJ"
      },
      "outputs": [],
      "source": [
        "# create the dataset\n",
        "chia_eng_dataset = Dataset.from_pandas(pd.DataFrame(sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q_TExHeUlAJ",
        "outputId": "7569cfb0-4f9a-4a91-f784-53d668263fad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['tokens', 'ner_tags'],\n",
              "    num_rows: 12433\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chia_eng_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kB0mY2UUlAJ"
      },
      "outputs": [],
      "source": [
        "chia_eng_train_test = chia_eng_dataset.train_test_split(test_size=0.2)\n",
        "chia_eng_test_val = chia_eng_train_test[\"test\"].train_test_split(test_size=0.5)\n",
        "chia_eng_dataset = DatasetDict({\n",
        "    \"train\": chia_eng_train_test[\"train\"],\n",
        "    \"test\": chia_eng_test_val[\"test\"],\n",
        "    \"validation\": chia_eng_test_val[\"train\"]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qyGdEyDUlAK",
        "outputId": "2a5bacc1-cf44-453c-e7a6-b717959c91ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['tokens', 'ner_tags'],\n",
              "        num_rows: 9946\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['tokens', 'ner_tags'],\n",
              "        num_rows: 1244\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['tokens', 'ner_tags'],\n",
              "        num_rows: 1243\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chia_eng_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDjVzi_vUlAK"
      },
      "source": [
        "**Model Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O3UQxFcUlAK"
      },
      "outputs": [],
      "source": [
        "model_name = 'xlm-roberta-base'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGX6j0viUlAK",
        "outputId": "430d0e42-e993-4fee-b439-4c5e27163764",
        "colab": {
          "referenced_widgets": [
            "3afd26d741cb4995bbd8159bc6de9fb5"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3afd26d741cb4995bbd8159bc6de9fb5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<s>', '▁The', '▁AI', '▁master', '▁at', '▁', 'Université', '▁Paris', '-', 'S', 'ac', 'lay', '▁is', '▁very', '▁good', '</s>']\n"
          ]
        }
      ],
      "source": [
        "# get xlm-roberta tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# check the tokenizer\n",
        "tokens_ = tokenizer(\"The AI master at Université Paris-Saclay is very good\").tokens()\n",
        "print(tokens_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsoHNOqrUlAK"
      },
      "outputs": [],
      "source": [
        "# tokenize and align the labels in the dataset\n",
        "def tokenize_and_align_labels(sentence, flag = 'I'):\n",
        "    \"\"\"\n",
        "    Tokenize the sentence and align the labels\n",
        "    inputs:\n",
        "        sentence: dict, the sentence from the dataset\n",
        "        flag: str, the flag to indicate how to deal with the labels for subwords\n",
        "            - 'I': use the label of the first subword for all subwords but as intermediate (I-ENT)\n",
        "            - 'B': use the label of the first subword for all subwords as beginning (B-ENT)\n",
        "            - None: use -100 for subwords\n",
        "    outputs:\n",
        "        tokenized_sentence: dict, the tokenized sentence now with a field for the labels\n",
        "    \"\"\"\n",
        "    tokenized_sentence = tokenizer(sentence['tokens'], is_split_into_words=True, truncation=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, labels_s in enumerate(sentence['ner_tags']):\n",
        "        word_ids = tokenized_sentence.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # if the word_idx is None, assign -100\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # if it is a new word, assign the corresponding label\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(labels_s[word_idx])\n",
        "            # if it is the same word, check the flag to assign\n",
        "            else:\n",
        "                if flag == 'I':\n",
        "                    if label_list[labels_s[word_idx]].startswith('I'):\n",
        "                      label_ids.append(labels_s[word_idx])\n",
        "                    else:\n",
        "                      label_ids.append(labels_s[word_idx] + 1)\n",
        "                elif flag == 'B':\n",
        "                    label_ids.append(labels_s[word_idx])\n",
        "                elif flag == None:\n",
        "                    label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_sentence['labels'] = labels\n",
        "    return tokenized_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okyg7yKkUlAK",
        "outputId": "a146edfa-43ef-4ce8-af55-f88c844bcd8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(chia_eng_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vYJx12mUlAK",
        "outputId": "19e8ac2d-40cd-47a4-b1d1-9039953ce9d9",
        "colab": {
          "referenced_widgets": [
            "ac8d142ea8c84d4cb7ab76eaf95f1863",
            "01cec26da9d3472bb41682513cd6a247",
            "cd1e2dfe7fb2459c8866b76226fc268a"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac8d142ea8c84d4cb7ab76eaf95f1863",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01cec26da9d3472bb41682513cd6a247",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd1e2dfe7fb2459c8866b76226fc268a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 8\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# apply the function to the dataset\n",
        "chia_eng_dataset = chia_eng_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "chia_eng_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjUI_2ZIUlAL",
        "outputId": "0fe7af10-f269-4aad-f3ba-0214dd4254f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XLMRobertaForTokenClassification(\n",
            "  (roberta): XLMRobertaModel(\n",
            "    (embeddings): XLMRobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): XLMRobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x XLMRobertaLayer(\n",
            "          (attention): XLMRobertaAttention(\n",
            "            (self): XLMRobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): XLMRobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): XLMRobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): XLMRobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=17, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# import the model\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(entities_list), label2id=sel_ent, id2label=sel_ent_inv)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUPcO4RQUlAL"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUmHsEIfUlAL"
      },
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KirT1RybUlAL"
      },
      "outputs": [],
      "source": [
        "# define the training arguments\n",
        "args = TrainingArguments(\n",
        "    \"chia-multilingual-ner\",\n",
        "    evaluation_strategy = \"steps\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    eval_steps=50,\n",
        "    save_steps=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifRS9_xkUlAL"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWz_1xCwUlAL"
      },
      "outputs": [],
      "source": [
        "metric = load_metric(\"seqeval\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cu4YtfUrUlAL"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p):\n",
        "    \"\"\"\n",
        "    Compute the metrics for the model\n",
        "    inputs:\n",
        "        p: tuple, the predictions and the labels\n",
        "    outputs:\n",
        "        dict: the metrics\n",
        "    \"\"\"\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpRKyCfFUlAL"
      },
      "outputs": [],
      "source": [
        "# define the trainer\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=chia_eng_dataset[\"train\"],\n",
        "    eval_dataset=chia_eng_dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBD3O2gQUlAL"
      },
      "outputs": [],
      "source": [
        "outputs_train = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BPELV2bUlAL"
      },
      "outputs": [],
      "source": [
        "print(outputs_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1A2mArQFUlAM"
      },
      "outputs": [],
      "source": [
        "outputs_eval = trainer.evaluate(chia_eng_dataset[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAYYVRsLUlAM"
      },
      "outputs": [],
      "source": [
        "print(outputs_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iqlN1QVUlAM",
        "outputId": "1afd9756-65b3-4fc6-b0bd-a527cad1455a"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "f-string: expecting '}' (2477619478.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[109], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    torch.save(model, f\"{chia-multilingual-ner\")\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: expecting '}'\n"
          ]
        }
      ],
      "source": [
        "torch.save(model, f\"{models_path}/chia-multilingual-ner.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfInLYObUlAM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}